{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars 🚗\n**Author:** [Greg Surma](https://towardsdatascience.com/@gsurma)<br/>\n**Original article:** [towardsdatascience.com](https://towardsdatascience.com/street-lanes-finder-detecting-street-lanes-for-self-driving-cars-fe069ec5a22d)",
      "metadata": {
        "cell_id": "00000-c28be645-9947-41ae-ab40-ac1cf276bf23",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In today’s article, we are going to use basic **Computer Vision** techniques to approach the street lanes detection problem which is crucial for self-driving cars. By the end of this article, you will be able to perform real-time lane detection with **Python** and **OpenCV**.\n\n![](car.gif)",
      "metadata": {
        "tags": [],
        "cell_id": "00001-49fc5122-cc1e-4c99-a155-555be4284b56",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## \n\nLet’s start with defining our problem.\n\nGiven an image of the road, we would like to detect street lanes on it.\n\n\n\nFirstly, we should import some useful libraries and create basic functions",
      "metadata": {
        "tags": [],
        "cell_id": "00002-bee26f64-5c46-4cdb-b5ce-6dfeca5f7667",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-2f65014c-5597-445a-bba0-cb26a9e00cc0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": null,
        "execution_millis": 18009,
        "execution_start": 1612181022200,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "!apt update && apt install -y python3-opencv\n!pip install opencv-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-ea347724-3c32-4e50-98d3-8e5fc1c003d9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "947e0b5a",
        "execution_millis": 1229,
        "execution_start": 1612181044379,
        "deepnote_cell_type": "code"
      },
      "source": "import cv2\nimport numpy as np\nimport os\nimport subprocess\nimport glob\nfrom matplotlib import pyplot as plt\n\nINPUT_FOLDER = \"./input/\"\nOUTPUT_FOLDER = \"./output/\"\nTEST_IMAGE = \"test_image.jpg\"\nTEST_VIDEO = \"test_video.mov\"\nDEBUG = False\n\ndef plot_image(image, title):\n    plt.imshow(image, cmap=plt.cm.gray)\n    plt.title(title)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Load the original image",
      "metadata": {
        "tags": [],
        "cell_id": "00005-111f7188-1b34-47ee-973d-bbf38fa92bb1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-019ccd86-78bd-4536-a714-15bc72847552",
        "deepnote_cell_type": "code"
      },
      "source": "original_image = cv2.imread(INPUT_FOLDER + TEST_IMAGE)\nplot_image(original_image, 'Original image')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Grayscale\n\n## \n\nThe first step of our pipeline is an image conversion from color to grayscale. We do it because in our case color values don’t hold any valuable information so to make further processing simpler and faster, we convert from three channels to one.",
      "metadata": {
        "tags": [],
        "cell_id": "00007-7d198dd8-f57c-4dbe-8536-ef91b4ca0f73",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-dda0d06c-73ed-4d33-ba89-a227d6a6b7d6",
        "deepnote_cell_type": "code"
      },
      "source": "test_image = cv2.imread(INPUT_FOLDER + TEST_IMAGE)\ntest_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\nplot_image(test_image, 'Grayscale image')\n     ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Blur\n\n### \n\nThen, to make our image smoother and less noisy, we apply a gentle Gaussian Blur.\n\nGaussian Blur works by calculating every pixel’s value as a weighted average of the surrounding pixels.\n\n![](blur.jpeg)",
      "metadata": {
        "tags": [],
        "cell_id": "00009-1f57f485-4b35-40dc-b652-3105c2c4e34c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-9e9f038b-9452-4af7-b1ef-99fb2f01bbe7",
        "deepnote_cell_type": "code"
      },
      "source": "test_image = cv2.GaussianBlur(test_image, (3,3),0)\nplot_image(test_image, 'Blur image')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Canny (Edge Detection)\n\n### \n\nThis is where the fun part begins. Now having a smooth grayscale image, we need to detect edges within it.\n\nFYI, it’s called ‘canny’ because it was invented by [John Canny](https://en.wikipedia.org/wiki/John_Canny?source=post_page---------------------------).\n\nWithout going into too many details, the core part of the canny edge detector is based on scanning the image and calculating derivatives (gradients) of neighboring pixel values. The higher the gradient, the more likely it is an edge.\n\n![An Example of a High Gradient Area](screenshot.png)",
      "metadata": {
        "tags": [],
        "cell_id": "00011-821a7e78-5e90-4164-8196-daf8045f9ea6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-870cd98a-004a-40e9-8d2f-87185ed0bd13",
        "deepnote_cell_type": "code"
      },
      "source": "test_image = cv2.Canny(test_image, 100, 150)\nplot_image(test_image, 'Canny image')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Region of Interest\n\n### \n\nNow, having our edges detected, we can clearly see where our street lanes are, but besides that, we can also see other edges that are redundant. In order to get rid of them, we should mask our image to a specific region that’s called a **Region of Interest (ROI)**. Defining proper ROI is highly dependant on the camera calibration and its frame i.e what part of the road is visible.",
      "metadata": {
        "tags": [],
        "cell_id": "00013-5524cbc7-511f-4fef-931f-1101c32c7410",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-7169fe70-00b7-4f07-a570-f3f299d25d10",
        "deepnote_cell_type": "code"
      },
      "source": "def roi(image):\n    bottom_padding = 100 # Front bumper compensation\n    height = image.shape[0]\n    width = image.shape[1]\n    # FYI, below values are highly dependant on the camera calibration i.e what part of the road is actually being captured\n    bottom_left = [0, height-bottom_padding]\n    bottom_right = [width, height-bottom_padding]\n    top_right = [width*1/3, height*1/3]\n    top_left = [width*2/3, height*1/3]\n    vertices = [np.array([bottom_left, bottom_right, top_left, top_right], dtype=np.int32)]\n    mask = np.zeros_like(image)  \n    cv2.fillPoly(mask, vertices, 255)\n    plot_image(mask, \"mask\")\n    masked_image = cv2.bitwise_and(image, mask)\n    return masked_image",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### This is the defined mask where the white color shows ROI",
      "metadata": {
        "tags": [],
        "cell_id": "00015-28fa351b-86ae-46fd-bc48-e18b56a78ba3",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00016-031d3ae6-eac9-4bc2-8248-e359c40f19d4",
        "deepnote_cell_type": "code"
      },
      "source": "roi_image = roi(test_image)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### this is canny image with a masked region of interest.",
      "metadata": {
        "tags": [],
        "cell_id": "00017-0c873a53-2b52-4b65-a439-dceedba0f6db",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-3c62d22a-d50d-43ae-be1d-ed5a9f68b53d",
        "deepnote_cell_type": "code"
      },
      "source": "plot_image(roi_image, \"Masked image\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Hough Lines\n\n### \n\nNow we have well defined lines that show where our street lanes are located. However, presenting them on the screen wouldn’t look appealing as they are noisy andflickery. In order to visualize them as single lines, we need to perform hough lines transformation.\n\n1. Firstly, let’s detect all lines. We define a line as [x1,y1,x2,y2] where (x1, y1) is its start and (x2, y2) is its end.\n2. Having our lines localized, we can calculate their slopes to determine whether they are right or left ones.\n3. Then we need to average lines and derive single left and right line.\n4. Ultimately, we can draw our final lines.",
      "metadata": {
        "tags": [],
        "cell_id": "00019-f0d992bd-b3ac-4de2-a10d-a3aee40ff2e0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00020-db842272-b46d-4e97-9374-7ce5c7fd9b92",
        "deepnote_cell_type": "code"
      },
      "source": "def averaged_lines(image, lines):\n    right_lines = []\n    left_lines = []\n    for x1,y1,x2,y2 in lines[:, 0]:\n        parameters = np.polyfit((x1, x2), (y1, y2), 1)\n        slope = parameters[0]\n        intercept = parameters[1]\n        if slope >= 0: \n            right_lines.append([slope, intercept])\n        else:\n            left_lines.append([slope, intercept])\n            \n    def merge_lines(image, lines):\n        if len(lines) > 0:\n            slope, intercept = np.average(lines, axis=0)\n            y1 = image.shape[0]\n            y2 = int(y1*(1/2))\n            x1 = int((y1 - intercept)/slope)\n            x2 = int((y2 - intercept)/slope)\n            return np.array([x1, y1, x2, y2])\n        \n    left = merge_lines(image, left_lines)\n    right = merge_lines(image, right_lines)\n    return left, right\n\ndef hough_lines(image, rho, theta, threshold, min_line_len, max_line_gap):\n    lines_image = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n    lines = cv2.HoughLinesP(image, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n    if lines is not None:\n        lines = averaged_lines(image, lines)\n        for line in lines:\n            if line is not None:\n                x1,y1,x2,y2 = line\n                cv2.line(lines_image, (x1, y1), (x2, y2), (0, 0, 255), 20)\n        plot_image(lines_image, \"lines\")\n    return lines_image",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00021-580a9570-c8bc-4679-83ca-4945a016d917",
        "deepnote_cell_type": "code"
      },
      "source": "hough_lines_image = hough_lines(roi_image, 0.9, np.pi/180, 100, 100, 50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Image Overlay\n\n### \n\nFinally, we need to overlay our input image with derived lines.",
      "metadata": {
        "tags": [],
        "cell_id": "00022-e41ecf4c-bd6f-4cd8-82c5-59925e10895d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00023-158e2cb3-19de-4f17-b64a-cfb6b0456dc1",
        "deepnote_cell_type": "code"
      },
      "source": "def combine_images(image, initial_image, α=0.9, β=1.0, λ=0.0):\n    combined_image = cv2.addWeighted(initial_image, α, image, β, λ)\n    plot_image(combined_image, \"combined\")\n    return combined_image",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00024-94b5d599-252a-414c-b189-eef6b574f12b",
        "deepnote_cell_type": "code"
      },
      "source": "final_image = combine_images(hough_lines_image, original_image)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Real-Time Detection\n\n### \n\nHaving a pipeline that can detect lines for a single frame, we can run it real-time on a video stream performing detections on every frame. Unfortunately, we need better hardware for it.\n\n![](car.gif)",
      "metadata": {
        "tags": [],
        "cell_id": "00025-da1d0d85-dd38-4553-82fe-6acb7f56ecbb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## What’s Next?\n\n### \n\nIn this project, we’ve learned how to use basic **Computer Vision** techniques for real-life problems. Even though our results look very promising, street lanes detector is far from perfection and it might fail in some cases. That is why in the next part we are going to use Deep Learning approach (**Holistically-Nested Edge Detection**) to achieve better accuracy and more reliable detector. Stay tuned!\n\n## \n\nQuestions? Comments? Feel free to leave your feedback in the comments section or contact me directly at [https://gsurma.github.io](https://gsurma.github.io/?source=post_page---------------------------).\n\nAnd don’t forget to 👏 if you enjoyed this article 🙂.",
      "metadata": {
        "tags": [],
        "cell_id": "00026-70ad98fd-0c07-4dd3-92af-cf92728a71b2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=67551570-9d71-4aed-9c87-2895f744897d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "deepnoteSessionId": "95cf77a5-ab3c-4ed5-b39b-f54c9a3f1b37",
    "deepnote_notebook_id": "0d0265cb-8baf-4ec4-bddc-e17571bedf00",
    "deepnote_execution_queue": [],
    "deepnote": {}
  }
}